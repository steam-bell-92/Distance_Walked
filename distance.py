# -*- coding: utf-8 -*-
"""Distance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sqoX-tDKZqC7Ztn54tNKW1uPb3u-1IKY

# ***Explorarory Data Analysis (EDA)***

---

## Importing all libraries for data processing
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

"""### Uploading & Reading the csv file"""

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('Health_dataset.csv')
df.head()

"""Getting the shape of dataset"""

df.shape

"""### Replacing the 'NaN' values from Sleep Disorder to 'None'"""

df['Sleep Disorder'] = df['Sleep Disorder'].fillna('None')
df.head()

"""### Dropping the duplicate values"""

df.drop_duplicates()

"""---

## Introducing a new parameter: Disance_walked(km)

---
"""

df['Distance_walked(km)'] = df['Daily Steps'] * 0.00074
df.head()

"""## Statistical summary of dataset"""

df.describe()

"""---
## Introducing two new parameters:

- BP_min
- BP_max

---
"""

df['BP_min'] = df['Blood Pressure'].apply(lambda x: int(x.split('/')[0]))
df['BP_max'] = df['Blood Pressure'].apply(lambda x: int(x.split('/')[1]))
df.head()

"""### Dropping the Blood Pressure as its of no use now after getting new columns of BP_min & BP_max"""

df.drop('Blood Pressure', axis=1, inplace=True)
df.drop('Daily Steps', axis=1, inplace=True)
df.head()

"""### Information about dataset"""

df.info()

"""## Plotting a Histogram with KDE of Distance_walked(km)"""

sns.histplot(x = 'Distance_walked(km)', data=df, kde=True)
plt.title('Distribution of Distance Walked')
plt.xlabel('Distance (km)')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--')
plt.show()

"""---

### ***Insights of histplot***
-  It is approximately bell-shaped, but not perfectly normal
- There is a slight positive skew (right-skewed)
- The average person likely walks between 4.5 to 6 km
- Outliers with low activity (<3.5 km) or high activity (>6.5 km) are rare

---

## Correlation Matrix between all numeric type datas
"""

sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""---

### ***Insights on heatmap***

- Physical activity and walking strongly benefit cardiovascular and mental health.
- Stress and heart rate are key disruptors of both sleep quantity and quality.
- Blood pressure rises with age, while stress and low activity raise heart rate and possibly BP.
- Stress and BP are not directly correlated strongly in your data, but stress affects heart rate and sleep, which are indirect drivers of blood pressure.

---

## Plotted a Paiplot to cover all rest relations required
"""

sns.pairplot(df)
plt.show()

"""---

# ***Machine Learning***

---

## Importing all libraries needed to make prediction
"""

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression, ElasticNet
from sklearn.preprocessing import RobustScaler, OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import root_mean_squared_error, mean_absolute_error

"""### Delibrately I have imported 3 types of model
- Linear Regression
- Support Vector Regressor (SVR)
- Random Forest Regressor

## Converting Categorial data to indicator variables
"""

cols = ['Occupation', 'Sleep Disorder', 'BMI Category', 'Gender']
for i in cols:
  df = pd.get_dummies(df, columns=[i], drop_first=True)
df.head()

"""## Splitting the dataset in train & test sets"""

X = df.drop('Distance_walked(km)', axis=1)
y = df['Distance_walked(km)']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""## Scaling the data"""

scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""#### I have used RobustScaler instead of StandardScaler due to
- There might be few outliners
- I haven't removed the outliners
- Outliners would be crucial while training this model as this one is somehow medically related & it might remove those datapoints

---

## Linear Regression Model

---
"""

lr = LinearRegression()
enet  = ElasticNet(alpha=0.1, l1_ratio=0.5)
lr.fit(X_train_scaled, y_train)
y_pred1 = lr.predict(X_test_scaled)

"""### Calculating the cross validation score for linear regression"""

lrc = cross_val_score(lr, X_train_scaled, y_train, cv=10)
lrc

"""## Calulating MAE & RMSE as cv_score does give indication of how far off are predictions for Linear Regression

### Mean Absolute Error (MAE)
"""

mae_lr = mean_absolute_error(y_test, y_pred1)
mae_lr

"""### Root Mean Squared Error"""

rmse_lr = root_mean_squared_error(y_test, y_pred1)
rmse_lr

"""## Plotted a scatter plot to visually observe the variance (Linear Regression)"""

plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred1, alpha=0.7, color='tomato', label='predicted')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='actual')
plt.xlabel('Actual Distance Walked (km)')
plt.ylabel('Predicted Distance Walked (km)')
plt.title('Actual vs Predicted Distance Walked (km)s (Linear Regression)')
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend(loc='best')
plt.tight_layout()
plt.show()

"""---

### ***Insights on scatterplot of Linear Regression***
- The linear relationship between actual and predicted values is strong, especially in the mid-range (4.5–6.5 steps).
- Predictions at extreme values (e.g. ~3 or ~7 steps) show larger variance.

---

---

## Support Vector Regressor(SVR) Model

---
"""

svm = SVR(kernel='poly', C=3, epsilon=0.2, degree=3)
svm.fit(X_train_scaled, y_train)
y_pred2 = svm.predict(X_test_scaled)

"""### Calculating the cross validation score for SVR"""

svmc = cross_val_score(svm, X_train_scaled, y_train, cv=10)
svmc

"""## Calulating MAE & RMSE as cv_score does give indication of how far off are predictions for SVR

### Mean Absolute Error (MAE)
"""

mae_svr = mean_absolute_error(y_test, y_pred2)
mae_svr

"""### Root Mean Squared Error (RMSE)"""

rmse_svr = root_mean_squared_error(y_test, y_pred2)
rmse_svr

"""## Plotted a scatter plot to visually observe the variance (SVR)"""

plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred2, alpha=0.7, color='tomato', label='predicted')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='actual')
plt.xlabel('Actual Distance Walked (km)')
plt.ylabel('Predicted Distance Walked (km)')
plt.title('Actual vs Predicted Distance Walked (km) (SVR)')
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend(loc='best')
plt.tight_layout()
plt.show()

"""---

### ***Insights on scattreplot of SVR***
- The scatter is more tightly clustered along the dashed line compared to Linear Regression.
- Less spread for lower and higher step counts — i.e., better edge behavior.
- Predictions follow the actual values more smoothly, thanks to the polynomial kernel.

---

---

## Random Forest Regressor Model

---
"""

rfr = RandomForestRegressor(random_state=42)
rfr.fit(X_train_scaled, y_train)
y_pred3 = rfr.predict(X_test_scaled)

"""### Calculated the cross validation score for Random Forest Regressor"""

rfrc = cross_val_score(rfr, X_train_scaled, y_train, cv=10)
rfrc

"""## Calulating MAE & RMSE as cv_score does give indication of how far off are predictions for Random Forest Regressor

### Mean Absolute Error (MAE)
"""

mae_rfr = mean_absolute_error(y_test, y_pred3)
mae_rfr

"""### Root Mean Squared Error (RMSE)"""

rmse_rfr = root_mean_squared_error(y_test, y_pred3)
rmse_rfr

"""## Plotted a scatter plot to visually observe the variance (Random Forest Regressor)"""

plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred3, alpha=0.7, color='tomato', label='predicted')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='actual')
plt.xlabel('Actual Distance Walked (km)')
plt.ylabel('Predicted Distance Walked (km)')
plt.title('Actual vs Predicted Distance Walked (km) (Random Forest Regressor)')
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend(loc='best')
plt.tight_layout()
plt.show()

"""---

### ***Insights on scatterplot of Random Forest Regressor***
- Indicates high prediction accuracy for most points.
- The predicted points are consistently close to actual values, with little vertical spread (error).
- Unlike Linear Regression, which struggled at edges, Random Forest handles non-linearities and extremes better.

---

## Plotting a barplot to compare the metrics across 3 models trained
"""

models = ['Linear Regression', 'SVR (Poly)', 'Random Forest']
mae = [mae_lr, mae_svr, mae_rfr]
rmse = [rmse_lr, rmse_svr, rmse_rfr]
r2 = [lrc.mean(), svmc.mean(), rfrc.mean()]
x = np.arange(len(models))
width = 0.25

fig, ax = plt.subplots(figsize=(10,6))
ax.bar(x - width, mae, width, label='MAE')
ax.bar(x, rmse, width, label='RMSE')
ax.bar(x + width, r2, width, label='R² Score')

for i, v in enumerate(mae):
    ax.text(x[i] - width, v + 0.01, f'{v:.6f}', ha='center')
for i, v in enumerate(rmse):
    ax.text(x[i], v + 0.01, f'{v:.6f}', ha='center')
for i, v in enumerate(r2):
    ax.text(x[i] + width, v + 0.01, f'{v:.6f}', ha='center')


ax.set_ylabel('Metric Value')
ax.set_title('Model Comparison: MAE, RMSE, R²')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend()
plt.ylim(0, 1)
plt.tight_layout()
plt.savefig("model_metrics_comparison.png", bbox_inches="tight")
plt.show()

"""---

### ***Insights on barplot with all metrics***

- Random Forest is the top performer, with:<br>
  - Highest R² (0.9541) → Best at explaining variance<br>
  - Tied for lowest MAE and RMSE → Most accurate predictions
- SVR (Poly) performs similarly on errors, but slightly behind on R².
- Linear Regression is the weakest, with both highest errors and lowest fit.

---
"""